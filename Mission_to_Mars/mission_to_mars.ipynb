{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create a soup objet where to search\n",
    "from bs4 import BeautifulSoup as bs\n",
    "#to read the page\n",
    "import requests\n",
    "# we need splinter \n",
    "from splinter import Browser\n",
    "# to create sleeps we need\n",
    "import time\n",
    "# to scrape tables from web\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start splinter in OSX\n",
    "def init_browser():\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    return Browser(\"chrome\", **executable_path, headless = False)\n",
    "browser=init_browser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape NASA's Mars page\n",
    "\n",
    "Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "#Page to be scraped, the lates news I found, and choosed manualy\n",
    "nasa = \"https://mars.nasa.gov/news/\"\n",
    "#Lets us splinter at once. (When I tried using a request, the html had some problems who didn't throw us the full html)\n",
    "browser.visit(nasa)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "#Take the html into a soup object\n",
    "soup = bs(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "marsnews={}\n",
    "#Take the first title you find\n",
    "titulo = soup.find(\"div\",class_=\"list_text\").a.text\n",
    "#Take the firs article teaser you find\n",
    "parafo = soup.find(\"div\", class_=\"article_teaser_body\").text\n",
    "#add them into marsnews dict\n",
    "marsnews['titulo'] = titulo\n",
    "marsnews['parafo'] = parafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'titulo': \"The Detective Aboard NASA's Perseverance Rover\",\n 'parafo': 'An instrument called SHERLOC will, with the help of its partner WATSON, hunt for signs of ancient life by detecting organic molecules and minerals.'}"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "marsnews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "# JPL Mars Space Images - Featured Image\n",
    "\n",
    "Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_img = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(nasa_img)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "# \n",
    "soup = bs(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/spaceimages/images/largesize/PIA23932_hires.jpg\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA23932_hires.jpg'"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "#Search for the link\n",
    "img_link = soup.find(\"section\", class_=\"grid_gallery module grid_view\").ul.li.a[\"data-fancybox-href\"]\n",
    "print(img_link)\n",
    "#The given link was incomplete so... \n",
    "#I found this requests function that allows to join an absolute path to the relative path within \n",
    "img_link = requests.compat.urljoin(nasa_img, img_link)\n",
    "#So here we have the complete link\n",
    "img_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather\n",
    "\n",
    "Visit the Mars Weather twitter account here and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New browser visit\n",
    "nasa_twitt = \"https://twitter.com/marswxreport?lang=en\"\n",
    "browser.visit(nasa_twitt)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "#New soup\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "InSight sol 533 (2020-05-27) low -92.8ºC (-135.1ºF) high -6.7ºC (19.9ºF)\nwinds from the SW at 6.4 m/s (14.2 mph) gusting to 21.3 m/s (47.5 mph)\npressure at 7.20 hPa\n"
    }
   ],
   "source": [
    "#Get the latest tweet about mars weather\n",
    "tweets_page = soup.main.section.div.article.find(\"div\", {\"lang\":\"en\"}).string\n",
    "print(tweets_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts\n",
    "\n",
    "Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_page = \"https://space-facts.com/mars/\"\n",
    "browser.visit(facts_page)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "#Soup\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to make posible to pandas to read a soup element i had to transform the string into a prettify\n",
    "table_loc = soup.section.article.find(\"div\", id=\"text-75\").table.prettify()\n",
    "#now we can use \"read_html\" from pandas\n",
    "table = pd.read_html(table_loc)\n",
    "#But to use it for our next page we have to convert it again into html\n",
    "#First I transform to a DataFrame\n",
    "table = pd.DataFrame(table[0])\n",
    "#And then to a html, without index and header\n",
    "table = table.to_html(index=False, header=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres\n",
    "\n",
    "Visit the USGS Astrogeology site here to obtain high resolution images for each of Mar's hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemispage = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(hemispage)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "#Soup\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg\n"
    }
   ],
   "source": [
    "hemisphere_image_url = {}\n",
    "#First we get in a list the foour different hemispheres representation\n",
    "hemispheres = soup.section.find(\"div\", class_=\"collapsible results\").find_all(\"div\", class_=\"item\")\n",
    "\n",
    "link = hemispheres[0].a[\"href\"]\n",
    "link = requests.compat.urljoin(hemispage, link)\n",
    "browser.visit(link)\n",
    "time.sleep(2)\n",
    "    # Read the html of the new page\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "    #\n",
    "image = soup.div.ul.li.a[\"href\"]\n",
    "print(image)\n",
    "\n",
    "browser.back()\n",
    "\n",
    "#siguiente obtener el titulo y hacer un replace del enhaced\n",
    "\n",
    "\n",
    "# for x in hemispheres:\n",
    "#     #Take the href link and complete it\n",
    "#     link = x.a[\"href\"]\n",
    "#     link = requests.compat.urljoin(hemispage, link)\n",
    "#     #Visit the new page\n",
    "#     browser.visit(link)\n",
    "#     time.sleep(2)\n",
    "#     # Read the html of the new page\n",
    "#     html = browser.html\n",
    "#     soup = bs(html, \"html.parser\")\n",
    "#     #\n",
    "#     image = soup.div.ul.li.a[\"href\"]\n",
    "\n",
    "#     browser.back()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}