{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create a soup objet where to search\n",
    "from bs4 import BeautifulSoup as bs\n",
    "#to read the page\n",
    "import requests\n",
    "# we need splinter \n",
    "from splinter import Browser\n",
    "# to create sleeps we need\n",
    "import time\n",
    "# to scrape tables from web\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start splinter in OSX\n",
    "def init_browser():\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    return Browser(\"chrome\", **executable_path, headless = False)\n",
    "browser=init_browser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape NASA's Mars page\n",
    "\n",
    "Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "#Page to be scraped, the lates news I found, and choosed manualy\n",
    "nasa = \"https://mars.nasa.gov/news/\"\n",
    "#Lets us splinter at once. (When I tried using a request, the html had some problems who didn't throw us the full html)\n",
    "browser.visit(nasa)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "#Take the html into a soup object\n",
    "soup = bs(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "marsnews={}\n",
    "#Take the first title you find\n",
    "titulo = soup.find(\"div\",class_=\"list_text\").a.text\n",
    "#Take the firs article teaser you find\n",
    "parafo = soup.find(\"div\", class_=\"article_teaser_body\").text\n",
    "#add them into marsnews dict\n",
    "marsnews['titulo'] = titulo\n",
    "marsnews['parafo'] = parafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'titulo': \"The Detective Aboard NASA's Perseverance Rover\",\n 'parafo': 'An instrument called SHERLOC will, with the help of its partner WATSON, hunt for signs of ancient life by detecting organic molecules and minerals.'}"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "marsnews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "# JPL Mars Space Images - Featured Image\n",
    "\n",
    "Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_img = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(nasa_img)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "# new soup\n",
    "soup = bs(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/spaceimages/images/largesize/PIA23932_hires.jpg\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA23932_hires.jpg'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#Search for the link\n",
    "img_link = soup.find(\"section\", class_=\"grid_gallery module grid_view\").ul.li.a[\"data-fancybox-href\"]\n",
    "print(img_link)\n",
    "#The given link was incomplete so... \n",
    "#I found this requests function that allows to join an absolute path to the relative path within \n",
    "img_link = requests.compat.urljoin(nasa_img, img_link)\n",
    "#So here we have the complete link\n",
    "img_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather\n",
    "\n",
    "Visit the Mars Weather twitter account here and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New browser visit\n",
    "nasa_twitt = \"https://twitter.com/marswxreport?lang=en\"\n",
    "browser.visit(nasa_twitt)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "#New soup\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'InSight sol 534 (2020-05-27) low -92.8ºC (-135.1ºF) high 0.9ºC (33.7ºF)\\nwinds from the SW at 4.8 m/s (10.8 mph) gusting to 18.8 m/s (42.1 mph)\\npressure at 7.20 hPa'"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "#The page has the problem that sometimes appears foreign tweets, that doesn't have de info we want\n",
    "#So first we have to take all the tweets\n",
    "tweets = soup.main.section.div.find_all(\"div\", {\"data-testid\" : \"tweet\"})\n",
    "\n",
    "#Then we iterate through tweets \n",
    "for tweet in tweets:\n",
    "    # Make sure the tweet is from Mars Weather\n",
    "    author = tweet.span.string\n",
    "    if author == \"Mars Weather\":\n",
    "        #If it is, take the text of the tweet\n",
    "        mars_weather = tweet.find(\"div\", {\"lang\":\"en\"}).span.string.strip()\n",
    "        #Stop iterate\n",
    "        break\n",
    "mars_weather\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts\n",
    "\n",
    "Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_page = \"https://space-facts.com/mars/\"\n",
    "browser.visit(facts_page)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "#Soup\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to make posible to pandas to read a soup element i had to transform the string into a prettify\n",
    "table_loc = soup.section.article.find(\"div\", id=\"text-75\").table.prettify()\n",
    "#now we can use \"read_html\" from pandas\n",
    "table = pd.read_html(table_loc)\n",
    "#But to use it for our next page we have to convert it again into html\n",
    "#First I transform to a DataFrame\n",
    "table = pd.DataFrame(table[0])\n",
    "#And then to a html, without index and header\n",
    "table = table.to_html(index=False, header=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres\n",
    "\n",
    "Visit the USGS Astrogeology site here to obtain high resolution images for each of Mar's hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemispage = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(hemispage)\n",
    "time.sleep(2)\n",
    "html = browser.html\n",
    "#Soup\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[{'title': 'Cerberus Hemisphere ', 'img_link': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'}, {'title': 'Schiaparelli Hemisphere ', 'img_link': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'}, {'title': 'Syrtis Major Hemisphere ', 'img_link': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'}, {'title': 'Valles Marineris Hemisphere ', 'img_link': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]\n"
    }
   ],
   "source": [
    "hemisphere_image_url = [] \n",
    "#First we get in a list the four parts with the different hemispheres \n",
    "hemispheres = soup.section.find(\"div\", class_=\"collapsible results\").find_all(\"div\", class_=\"item\")\n",
    "\n",
    "#Then we iterate through the 2 hemispheres\n",
    "for x in hemispheres:\n",
    "    #Take the href link to the page who has the image and complete it\n",
    "    link = x.a[\"href\"]\n",
    "    link = requests.compat.urljoin(hemispage, link)\n",
    "    #Visit the new page\n",
    "    browser.visit(link)\n",
    "    time.sleep(2)\n",
    "    # Read the html of the new page\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    #Get the title and the links to image and put them in a dictionary\n",
    "    title = soup.section.h2.string.replace(\"Enhanced\", \"\")\n",
    "    links = soup.div.ul.li.a[\"href\"]\n",
    "    dictionary = {\"title\" : title , \"img_link\" : links }\n",
    "    #apend the dictionary to the list of urls\n",
    "    hemisphere_image_url.append(dictionary)\n",
    "    #Get back to the original page\n",
    "    browser.back()\n",
    "#check ifall right\n",
    "print(hemisphere_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}